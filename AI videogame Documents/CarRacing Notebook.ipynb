{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89230b0d",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d29520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59355d9f",
   "metadata": {},
   "source": [
    "Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #run on GPU is possible, otherwise on CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068e37a",
   "metadata": {},
   "source": [
    "Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8993ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_preprocessing(img): \n",
    "  img = cv2.resize(img, dsize=(84, 84)) #resizes the image to 84x84 pixels\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0 #Gray-scales the image\n",
    "  return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1ee66",
   "metadata": {},
   "source": [
    "Environment Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CarEnvironment(gym.Wrapper):\n",
    "  def __init__(self, env, skip_frames=4, stack_frames=4, no_operation=50, **kwargs):\n",
    "    super().__init__(env, **kwargs)\n",
    "    self._no_operation = no_operation   #randomize initial state \n",
    "    self._skip_frames = skip_frames   #skip frames to reduce computation resource need\n",
    "    self._stack_frames = stack_frames    #stack frames for temporal context\n",
    "\n",
    "  def reset(self):\n",
    "    observation, info = self.env.reset()\n",
    "    #Perform no-op actions to vary starting state\n",
    "    for i in range(self._no_operation):\n",
    "      observation, reward, terminated, truncated, info = self.env.step(0)\n",
    "    #Preprocess and stack initial frames\n",
    "    observation = image_preprocessing(observation)\n",
    "    self.stack_state = np.tile(observation, (self._stack_frames, 1, 1))\n",
    "    return self.stack_state, info\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    total_reward = 0\n",
    "    for i in range(self._skip_frames):\n",
    "      observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "      total_reward += reward\n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "          #Preprocess and update stacked frames\n",
    "    observation = image_preprocessing(observation)\n",
    "    self.stack_state = np.concatenate((self.stack_state[1:], observation[np.newaxis]), axis=0)\n",
    "    return self.stack_state, total_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b0395",
   "metadata": {},
   "source": [
    "Actor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self._n_features = 32 * 9 * 9\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(self._n_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, out_channels),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = x.view((-1, self._n_features))\n",
    "    x = self.fc(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b55d5",
   "metadata": {},
   "source": [
    "Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self._n_features = 32 * 9 * 9\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(self._n_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, out_channels),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = x.view((-1, self._n_features))\n",
    "    x = self.fc(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d8292",
   "metadata": {},
   "source": [
    "PPO Class\n",
    "PPO - initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d13e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPO:\n",
    "  def __init__(self, action_dim=5, obs_dim=4, trajectories=512, gamma=0.99, lr_actor=3e-4, lr_critic=1e-3, clip=0.2, n_updates=10, lambda_=0.99,\n",
    "               moving_avg_window=100, convergence_threshold=0.01, check_every=50, patience=10, max_episodes=100000):\n",
    "    self.action_dim = action_dim\n",
    "    self.obs_dim = obs_dim\n",
    "    self.trajectories = trajectories\n",
    "    self.gamma = gamma\n",
    "    self.lr_actor = lr_actor\n",
    "    self.lr_critic = lr_critic\n",
    "    self.clip = clip\n",
    "    self.n_updates = n_updates\n",
    "    self.lambda_ = lambda_\n",
    "    self._total_rewards = []\n",
    "    self.actor = Actor(obs_dim, action_dim).to(device)\n",
    "    self.critic = Critic(obs_dim, 1).to(device)\n",
    "    self.actor_optim = Adam(self.actor.parameters(), lr=self.lr_actor)\n",
    "    self.critic_optim = Adam(self.critic.parameters(), lr=self.lr_critic)\n",
    "    \n",
    "    # Convergence parameters\n",
    "    self.moving_avg_window = moving_avg_window  \n",
    "    self.convergence_threshold = convergence_threshold\n",
    "    self.check_every = check_every             \n",
    "    self.patience = patience                   \n",
    "    self.max_episodes = max_episodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61e111",
   "metadata": {},
   "source": [
    "Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "  \"\"\"\n",
    "  This function takes as a parameter an observation, feeds it to the CNN and gets the\n",
    "  raw predictions (logits) and it samples an action through the categorical distribution.\n",
    "  It returns the action and the logarithmic probability.\n",
    "  \"\"\"\n",
    "  def get_action(self, obs):\n",
    "    # Our observation is a 2D numpy array so we first create a tensor\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Feeding the tensor to the actor and get the logits\n",
    "    action_probs = self.actor(obs)\n",
    "\n",
    "    # Creating a Categorical distribution\n",
    "    dist = Categorical(logits=action_probs)\n",
    "\n",
    "    # Sampling the action\n",
    "    action = dist.sample()\n",
    "\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    return action.detach().cpu().numpy(), log_prob.detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac3955",
   "metadata": {},
   "source": [
    "Trajectory Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \"\"\"\n",
    "  This function is where we collect the trajectories (e.g. observations, rewards and other information)\n",
    "  using the current policy. We run this until we collect the number of trajectories we set.\n",
    "  It returns all the collected information.\n",
    "  \"\"\"\n",
    "  def collect_trajectories(self):\n",
    "    batch_obs = []\n",
    "    batch_rewards = []\n",
    "    batch_log_probs = []\n",
    "    batch_next_obs = []\n",
    "    batch_actions = []\n",
    "    batch_dones = []\n",
    "    t = 0\n",
    "\n",
    "    # Creating the discrete environment and passing it through the our wrapper for the modification\n",
    "    env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "    env = CarEnvironment(env)\n",
    "\n",
    "    while True:\n",
    "\n",
    "      # Reset environment\n",
    "      obs, _ = env.reset()\n",
    "\n",
    "      # Runs as many times as needed until we get the number of trajectories we want\n",
    "      while True:\n",
    "\n",
    "        # Append current state\n",
    "        batch_obs.append(obs)\n",
    "\n",
    "        # Choose an action\n",
    "        a, log_prob = self.get_action(obs)\n",
    "\n",
    "        # Append action\n",
    "        batch_actions.append(a)\n",
    "\n",
    "        # Append log prob\n",
    "        batch_log_probs.append(log_prob)\n",
    "\n",
    "        # Perform the action\n",
    "        obs, rew, terminated, truncated, _ = env.step(a.item())\n",
    "\n",
    "        # Append reward\n",
    "        batch_rewards.append(rew)\n",
    "\n",
    "        # Increase the number of T horizon\n",
    "        t += 1\n",
    "\n",
    "        # Check criterion for loop termination\n",
    "        if terminated or truncated or t == self.trajectories:\n",
    "          batch_dones.append(1)\n",
    "          break\n",
    "        else:\n",
    "          batch_dones.append(0)\n",
    "\n",
    "      # Check criterion for loop termination\n",
    "      if t == self.trajectories:\n",
    "        env.close()\n",
    "        break\n",
    "\n",
    "    self._total_rewards.append(sum(batch_rewards))\n",
    "\n",
    "    # Convert to tensors\n",
    "    batch_obs = np.array(batch_obs)\n",
    "    batch_obs = torch.tensor(batch_obs, dtype=torch.float32)\n",
    "    batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32)\n",
    "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float32)\n",
    "    batch_actions = torch.tensor(batch_actions, dtype=torch.long)\n",
    "\n",
    "    # Reward Normalization\n",
    "    batch_rewards = (batch_rewards - batch_rewards.mean()) / (batch_rewards.std() + 1e-8)\n",
    "\n",
    "    return batch_obs, batch_rewards, batch_log_probs, batch_actions, batch_dones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb8718",
   "metadata": {},
   "source": [
    "Discounted return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99223d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \"\"\"\n",
    "  Computing the discounted reward sum based on the current V values with\n",
    "  GAE (Generalized Advantage Estimation)\n",
    "  \"\"\"\n",
    "  def compute_discounted_sum(self, batch_rewards, V, batch_dones):\n",
    "    discounted_sum = []\n",
    "    gae = 0\n",
    "    zero = torch.tensor([0])\n",
    "    V = torch.cat((V.cpu(), zero))\n",
    "\n",
    "    for i in reversed(range(len(batch_rewards))):\n",
    "      #TD error with bootstraped value\n",
    "      delta = batch_rewards[i] + self.gamma * V[i + 1] * (1 - batch_dones[i]) - V[i]\n",
    "      #GAE update with decay\n",
    "      gae = delta + self.gamma * self.lambda_ * gae * (1 - batch_dones[i])\n",
    "      discounted_sum.insert(0, gae)\n",
    "\n",
    "    return discounted_sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe47908",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \"\"\"\n",
    "  Make the agent learn the environment\n",
    "  \"\"\"\n",
    "  def train(self):\n",
    "      convergence_counter = 0\n",
    "      episode = 0\n",
    "      best_avg_reward = float('-inf')\n",
    "      training_start_time = time.time()\n",
    "      recent_averages = []  # Keep track of recent moving averages\n",
    "\n",
    "      while episode < self.max_episodes:\n",
    "          episode += 1\n",
    "\n",
    "          # Every 10 episodes, print rewards\n",
    "          if episode % 10 == 0:\n",
    "              current_avg = np.mean(self._total_rewards[-20:]) if len(self._total_rewards) >= 20 else float('-inf')\n",
    "              print(f\"Episode {episode} - Average Reward (last 20 episodes): {current_avg:.2f}\")\n",
    "\n",
    "          # Save periodically\n",
    "          if episode % 500 == 0:\n",
    "              print(\"Processed: \", episode)\n",
    "              torch.save(self.actor.state_dict(), f'actor_weights_{episode}.pth')\n",
    "              torch.save(self.critic.state_dict(), f'critic_weights_{episode}.pth')\n",
    "              with open('statistics.pkl', 'wb') as f:\n",
    "                  pickle.dump((self._total_rewards), f)\n",
    "\n",
    "          # Collect trajectories and train as before\n",
    "          batch_obs, batch_rewards, batch_log_probs, batch_actions, batch_dones = self.collect_trajectories()\n",
    "          \n",
    "          # Compute critic values\n",
    "          V = self.critic(batch_obs.to(device)).squeeze()\n",
    "\n",
    "          # Rest of the training logic remains the same until convergence checking\n",
    "          discounted_sum = self.compute_discounted_sum(batch_rewards, V, batch_dones)\n",
    "          discounted_sum = torch.tensor(discounted_sum, dtype=torch.float32)\n",
    "          advantages = discounted_sum - V.detach().cpu()\n",
    "          advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "          for update in range(self.n_updates):\n",
    "              actions_probs = self.actor(batch_obs.to(device))\n",
    "              action_log_probs = actions_probs.gather(1, batch_actions.to(device)).squeeze()\n",
    "              ratios = torch.exp(action_log_probs - batch_log_probs.to(device)).cpu()\n",
    "              surr1 = ratios * advantages\n",
    "              surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * advantages\n",
    "              loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "              self.actor_optim.zero_grad()\n",
    "              loss.backward(retain_graph=True)\n",
    "              self.actor_optim.step()\n",
    "\n",
    "              V = self.critic(batch_obs.to(device)).squeeze()\n",
    "              value_loss = nn.MSELoss()(V, discounted_sum.detach().to(device))\n",
    "\n",
    "              self.critic_optim.zero_grad()\n",
    "              value_loss.backward()\n",
    "              self.critic_optim.step()\n",
    "\n",
    "          # Check for convergence\n",
    "          if episode > self.moving_avg_window and episode % self.check_every == 0:\n",
    "              recent_rewards = self._total_rewards[-self.moving_avg_window:]\n",
    "              current_avg = np.mean(recent_rewards)\n",
    "              recent_averages.append(current_avg)\n",
    "              \n",
    "              # Save best model if we have a new best average\n",
    "              if current_avg > best_avg_reward:\n",
    "                  best_avg_reward = current_avg\n",
    "                  torch.save(self.actor.state_dict(), 'best_actor.pth')\n",
    "                  torch.save(self.critic.state_dict(), 'best_critic.pth')\n",
    "              \n",
    "              # Check for convergence only if we have enough data\n",
    "              if len(recent_averages) >= 3:  # Need at least 3 points to check stability\n",
    "                  # Calculate relative changes between consecutive averages\n",
    "                  changes = [abs((recent_averages[i] - recent_averages[i-1]) / recent_averages[i-1]) \n",
    "                           for i in range(len(recent_averages)-2, len(recent_averages))]\n",
    "                  \n",
    "                  avg_change = np.mean(changes)\n",
    "                  print(f\"Convergence check - Current avg: {current_avg:.2f}, Average relative change: {avg_change:.2%}\")\n",
    "                  \n",
    "                  # Check if changes are consistently small\n",
    "                  if avg_change < self.convergence_threshold:\n",
    "                      convergence_counter += 1\n",
    "                      print(f\"Convergence counter: {convergence_counter}/{self.patience}\")\n",
    "                      if convergence_counter >= self.patience:\n",
    "                          print(f\"Model has converged at episode {episode}! Training complete.\")\n",
    "                          print(f\"Final average reward: {current_avg:.2f}\")\n",
    "                          self._save_final_data(episode, training_start_time)\n",
    "                          return\n",
    "                  else:\n",
    "                      convergence_counter = 0\n",
    "                      \n",
    "                  # Keep only recent history to avoid old averages affecting convergence check\n",
    "                  if len(recent_averages) > 10:\n",
    "                      recent_averages = recent_averages[-10:]\n",
    "\n",
    "      print(f\"Reached maximum episodes ({self.max_episodes}). Training stopped.\")\n",
    "      self._save_final_data(episode, training_start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bf2a5",
   "metadata": {},
   "source": [
    "Save Final Results in Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b18962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  def _save_final_data(self, final_episode, start_time):\n",
    "      \"\"\"Save final training data and statistics\"\"\"\n",
    "      training_time = time.time() - start_time\n",
    "      \n",
    "      # Save final model weights\n",
    "      torch.save(self.actor.state_dict(), 'final_actor.pth')\n",
    "      torch.save(self.critic.state_dict(), 'final_critic.pth')\n",
    "      \n",
    "      # Save all training data\n",
    "      training_data = {\n",
    "          'total_rewards': self._total_rewards,\n",
    "          'final_episode': final_episode,\n",
    "          'training_time': training_time,\n",
    "          'convergence_params': {\n",
    "              'moving_avg_window': self.moving_avg_window,\n",
    "              'convergence_threshold': self.convergence_threshold,\n",
    "              'check_every': self.check_every,\n",
    "              'patience': self.patience\n",
    "          },\n",
    "          'final_average_reward': np.mean(self._total_rewards[-self.moving_avg_window:]) if self._total_rewards else None\n",
    "      }\n",
    "      \n",
    "\n",
    "      # Save complete training statistics\n",
    "      with open('final_statistics.pkl', 'wb') as f:\n",
    "          pickle.dump(training_data, f)\n",
    "      \n",
    "      print(\"\\nTraining Summary:\")\n",
    "      print(f\"Total Episodes: {final_episode}\")\n",
    "      print(f\"Training Time: {training_time/3600:.2f} hours\")\n",
    "      print(f\"Final Average Reward: {training_data['final_average_reward']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd2bb8",
   "metadata": {},
   "source": [
    "Call Functions for PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PPO(max_episodes=100000)  # Training stops after 100000 episodes in case of convergence is not hit\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bce4dd",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d124246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create evaluation environment and wrap it\n",
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "eval_env = CarEnvironment(eval_env)\n",
    "\n",
    "frames = [] #store frames for video rendering\n",
    "scores = 0\n",
    "s, _ = eval_env.reset()\n",
    "\n",
    "done, ret = False, 0\n",
    "\n",
    "while not done:\n",
    "    frames.append(eval_env.render()) #Save frame for animation\n",
    "    s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    a = torch.argmax(model.actor(s), dim=-1) #Select best action\n",
    "    discrete_action = a.item() % 5 #Convert to discrete action\n",
    "    s_prime, r, terminated, truncated, info = eval_env.step(discrete_action)\n",
    "    s = s_prime\n",
    "    ret += r\n",
    "    done = terminated or truncated\n",
    "    if terminated:\n",
    "      print(terminated)\n",
    "scores += ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6a111",
   "metadata": {},
   "source": [
    "Video Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05749230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def animate(imgs, video_name, _return=True):\n",
    "    import cv2\n",
    "    import os\n",
    "    import string\n",
    "    import random\n",
    "\n",
    "    #Generate random video name\n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90') #use VP9 coded\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height)) #10 fps\n",
    "\n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #Convert color format\n",
    "        video.write(img) #Add frame to video\n",
    "    video.release() #Finalize file\n",
    "\n",
    "animate(frames, None) #Save animation from collected frames\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
